---
title: Findit - 쇼핑몰 스크래핑(크롤링) 실패기
date: 2025-11-24 16:00:00 +0900
categories: [Project, Browser-Extensions]
tags: [findit, backend, crawling, api]
---
<a href="https://github.com/victorybh04/findit/tree/main" target="_blank">Findit Github 링크</a>

Findit 프로젝트의 핵심 기능은 동일 상품을 다른 쇼핑몰에서 검색하여 가격을 띄워주는 것이다. 즉, 쇼핑몰에 검색을 요청하여 검색 결과(상품 리스트 페이지)를 받아낼 방법이 필요하다. 

## 스크래핑
쇼핑몰의 상품 정보를 가져올 방법으로 가장 처음으로 생각한 방식은 스크래핑이다.

스크래핑(Scraping)이란 봇을 이용하여 특정 웹 페이지에서 필요한 데이터를 자동으로 추출해내는 것을 의미한다. 크롤링(Crawling)과 혼동되곤 한다. (나 역시 처음에는 이 프로젝트에 필요햔 기술을 단순히 크롤링으로만 알고 있었다.) 크롤링은 여러 불특정 다수의 웹 페이지들을 찾아다니며 전체 내용을 수집하는 것과 달리, 스크래핑은 특정 웹 페이지에 대해서만 정보를 수집한다는 차이가 있다. 
이 프로젝트에서는 **쇼핑몰의 상품 검색 결과 단일 페이지**와 **상세 정보 페이지들**에 대해서만 정보를 수집하기 때문에 크롤링보다는 스크래핑이라는 단어가 적절해 보인다. 다만 중요한 기술적 개념 차이는 아니기에, 앞으로 포스트에서도 크게 구분하지 않고 혼용하여 사용하고자 한다.

문제는 많은 웹 페이지가 공인된 봇(GoogleBot, Twitterbot 등)들을 제외하고는 크롤링을 금지하고 있다는 것이다. 결론부터 이야기하면, **지마켓과 쿠팡을 대상으로 한 스크래핑 테스트는 실패했다.** `axios`와 `cheerio`라는 라이브러리를 이용하여 스크래핑을 시도한 과정을 설명하고자 한다.

## axios와 cheerio
해당 프로젝트의 백엔드 `server.js`에서 사용할 라이브러리로 `axios`와 `cheerio`를 선택했다. `axios` 라이브러리는 
> 브라우저와 Node.js를 위한 Promise 기반의 HTTP 비동기 통신 라이브러리입니다. 서버와 프론트엔드 간의 데이터를 주고받는 작업을 편리하게 만들어주며, 웹 개발에서 백엔드와 소통하는 데 널리 사용됩니다. 

이라고 한다. 사실 JS에는 `fetch API`를 이용한 Ajax 비동기 통신이 가능하다. 그럼에도 `axios`를 이용하는 이유는 다음과 같다.
1. Ajax를 구현할 때 많은 사람들이 습관적으로 사용한다. ~~(인기가 많다)~~

즉 기존에 사용하던 익숙함(관성) 때문에 계속 사용되는 경향이 크다. 심지어 `React`나 `Vue.js`와 같은 모던 프론트엔드 생태계에서도 `fetch`보다 `axios`를 기본으로 채택하는 경우가 많다. 이는 `axios` 라이브러리가 `fetch` 문법이 생기기 전부터 존재했던 라이브러리기에, 많은 사람들이 굳이 `fetch`로 넘어가지 않고 `axios`를 그대로 사용하는 것이다. 

2. 다양한 문법과 사용의 간결함. 특히 timeout 설정 문법이 간결하며, json 파싱 역시 자동으로 되는 `axios`가 편리하다.
3. 에러 처리 방식의 직관성. `axios`는 http 통신에서 상태 코드가 2XX(통신 성공) 범위를 벗어나면 바로 에러를 던지지만, `fetch`를 이용하면 400 ~ 500번대 에러가 떠도 네트워크 통신 자체는 성공했다고 판단하여 에러를 던지지 않는다. 


스크래핑(크롤링) 툴로써 사용한 `cheerio` 라이브러리는 
> Cheerio는 Node.js 환경에서 서버 측에서 웹 스크래핑을 할 때 사용되는 라이브러리로, jQuery와 유사한 API를 제공하여 HTML 및 XML 문서를 빠르고 유연하게 파싱하고 조작할 수 있게 해줍니다.

이라고 한다. 가장 큰 특징은 `jQuery`와 비슷한 문법을 통해 HTML의 요소를 조작할 수 있다는 점이다. 실제로 사용해보니, 사용법이 `jQuery`와 거의 유사하여 특별히 깊게 공부하지 않고도 사용할 수 있었다.


## `server.js` 구현
`server.js`의 구조를 나누어 정리하면 다음과 같다.

```js
const FAKE_USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36';

// ...

app.get('/search', async (req, res) => { // '/search' api endpoint
    const searchQuery = req.query.q;
    // ...
    try {
        const gmarketUrl = `https://browse.gmarket.co.kr/search?keyword=${encodeURIComponent(searchQuery)}`;
        const coupangUrl = `https://www.coupang.com/np/search?&q=${encodeURIComponent(searchQuery)}`;

        const response = await axios.get(gmarketUrl, {
            headers: {
                'User-Agent': FAKE_USER_AGENT,
                'Referer': 'https://www.gmarket.co.kr/'
            },
            timeout: 5000
        });

```
- `app`은 `express` 프레임워크로, 브라우저 익스텐션 팝업에서 해당 백엔드 서버로 요청(`/search` api 엔드포인트)을 받아 `req`의 `query.q`를 `searchQuery` 변수에 저장하게 된다. 
- `axios.get` 으로 `gmarketUrl` 주소로 요청을 보낸다. 
- `searchQuery`는 상품 검색어로, `encodeURIComponent()` 함수를 이용해 UTF-8 인코딩하여 쿼리로 사용할 수 있도록 하고, URL에 집어넣어 사용한다.
- `header`에 `User-Agent`를 `FAKE_USER_AGENT` 변수에 저장해둔 일반적인 크롬 유저 에이전트 값으로 설정하고, `referer`를 해당 쇼핑몰 홈페이지 주소로 설정하여 최대한 봇이 아닌것처럼 보이도록 한다. 
- `timeout` 5초 이상 응답이 없으면 자동으로 연결시도를 종료한다.

```js
        const html = response.data;
        const $ = cheerio.load(html);

        const productItems = $('div.box__item-container'); // 지마켓 검색결과 - 상품명 컨테이너 셀렉터
        const crawledResult = [];
```
- `html` 변수는 `axios.get`으로 받아온 `response`의 `data` 프로퍼티, 즉 HTML 데이터를 담는다.
- `cheerio`로 `html` 변수를 받아 `$`에 로드한다.
- `productItems`는 쇼핑몰 페이지의 상품명이 담긴 HTML요소를 받는다. 위 예제의 `div.box__item-container`는 지마켓의 상품명 컨테이너의 셀렉터이다. 해당 셀렉터를 가진 요소들이 여러개이므로 `productItems`는 배열 자료형이다.
- `crawledResult`에는 `cheerio`를 통해 분석한 html 요소들(쇼핑몰, 상품명, 가격, URL, 배송료 등)을 가져올 것이다.

```js
        productItems.each((index, element) => {
            const title = $(element).find('span.text__item').text().trim();
            const price = $(element).find('strong.text__value').text().trim();
            const url = $(element).find('a.link__item').attr('href');
            const shippingFeeText = $(element).find('span.text__tag').text().trim();

            crawledResult.push({
                storeName: ['G마켓', 'gmarket'],
                title: title,
                price: price,
                url: url,
                shippingFeeText: shippingFeeText
            });
        });
        res.json({ results: crawledResult });
    } 
```
- `productItems` 배열을 순회하는 반복문으로 각 상품 카드에 대한 정보들을 분석하여 객체에 담아 `crawledResult`에 담는다.
- `res.json({ results: crawledResult });` 으로 실제 스크래핑 결과를 팝업에 보낸다. 실제로는 가장 일치하는 상품을 찾아 하나만 보내도록 하는 로직이 추가되어야 할 것이다.

```js
    catch (error) {
        console.error('크롤링 중 오류 발생', error.message);
        res.json({ results: [] });
    }
});
```
- 에러가 발생하면 `error.message`를 콘솔에 반환하고, 팝업에는 빈 결과를 보낸다.


## 결과
위에서 이야기했듯, 지마켓과 쿠팡에 대한 스크래핑 시도는 처참히 실패로 끝났다... 

### 지마켓
![403-Error](/assets/img/findit/2-crawling-error-403.png){: w="400"}
_지마켓에 요청을 보낸 결과_
지마켓의 경우, 403 에러를 띄우며 작동하지 않았다. 403 에러는 서버에 요청은 도달했으나, **접근할 권한이  부족하다는 이유로 서버가 요청을 거부**하는 에러 코드이다. 아마 지마켓 서버에서는 일반적인 요청이 아니고, 봇의 접근이라 판단했기에 요청을 거부했을 것이다. 

그래서 최대한 '정상적인 접근'으로 보이기 위해 유저 에이전트와 리퍼러(어느 웹 페이지에서 접속을 요청하였는가를 나타내는 헤더)를 조작해 보았다. 이것이 위 코드에서 `axios.get`으로 요청을 보낼 때 헤더에 다음과 같은 코드를 추가한 이유이다. 
``` js
    headers: {
        'User-Agent': FAKE_USER_AGENT,
        'Referer': 'https://www.gmarket.co.kr/'
    }
```
결과는 마찬가지로 403 에러 코드를 띄우며 실패했다. 대부분의 크롤링을 막는 웹 페이지가 그렇듯, 단순히 유저 에이전트만 확인하는것이 아니라 스크립트를 실행할 수 있는지(JS 챌린지), 일반적인 IP 위치에서 요청되는지(클라우드 IP에서 요청이 오진 않았는지) 등의 방식으로 봇의 접근을 차단하고 있는 것으로 보인다.  

### 쿠팡
![read_ECONNRESET](/assets/img/findit/2-crawling-error-readECONNRESET.png){: w="400"}
_쿠팡에 요청을 보낸 결과_
쿠팡의 경우, 한동안 오류코드도 뜨지 않고 반응이 없더니, 한참 뒤에야 위 사진처럼 `read ECONNRESET` 오류를 띄웠다. 해당 오류를 찾아보니 TCP 통신이 끊겼을 때 나오는 에러라고 한다. 즉 쿠팡 서버에서는 해당 요청에 대해 아무런 응답도, 에러 코드도 띄우지 않고 무시한다는 것이다. 

이 경우 역시 실패지만, 만약 실제 서버 구동시 같은 문제가 발생할 때, 연결이 스스로 끊길때까지 기다리게 두면 필요없는 병목이 생길 수 있다는 것을 깨달았다. 그래서 `axios.get` 코드에 timeout 코드를 추가했다.

결론은 두 메이저한 쇼핑몰 모두 봇을 이용한 스크래핑이 불가능했다.

## 해결책
두 실패 모두 서버가 봇임을 감지하여 결과를 보내주지 않는 문제가 있었다. 이를 해결하기 위해 검색하던 중 puppeteer라는 라이브러리를 발견했다. 이는 실제로 브라우저를 동작하여 스크래핑과 크롤링을 도와주는 라이브러리로, html 파일만 조작할 수 있는 `cheerio`와 다르게 실제 브라우저와 동작 원리가 같기에 HTML과 JS 모두 다룰 수 있다. 즉 봇 문제를 회피하기에 최적인 라이브러리이다.

 `axios`와 `cheerio` 조합이 아니라 `puppeteer` 라이브러리를 이용하면 실제 크롬 브라우저로 위장하여 어떻게든 스크래핑이 가능할수도 있겠다. 하지만 다음과 같은 문제가 있다.

1. 백엔드 서버가 너무 무거워진다. 화면을 띄워주지 않을 뿐 실제로 브라우저를 구동하는 것과 다름없기에 실제 서버 구동을 위해서는 강력한 서버 컴퓨터가 필요할 것이다. ~~AWS나 GCP를 이용한다면 월 사용료가 수익을 넘어서는 수준일 것이다..~~
2. 이정도로 스크래핑을 막았다면, 기본적으로 해당 쇼핑몰들의 정책상 스크래핑이 강력히 금지된다는 뜻이다. 사실 기술적인 차단 이전에도 각 사이트의 `robots.txt`를 확인했을 때 기본적인 검색엔진 봇 외에는 봇 접근이 허용되지 않고 있었다. 지금 개인 프로젝트를 하면서 테스트를 해보는 정도에서는 문제가 되지 않겠지만, 만약 프로젝트가 완성되고 실제로 수익을 낼 수 있게 된다면 심각한 법적 문제가 생길지도 모른다. 기술적으로 뚫을 수 있다고 해도 윤리적으로나 법적으로나 문제가 될 소지가 다분하다.

그래서 생각해본 다른 해결책들은 다음과 같다.

### 1. Open API를 제공하는 다른 쇼핑몰
쿠팡과 지마켓도 API를 제공하지만, 내가 확인한 바로는 판매자를 위한 API일 뿐 상품을 검색하는 API는 존재하지 않았다. 즉 해당 방법은 쿠팡과 지마켓에 대한 방법은 되지 못할 것이다.

하지만 11번가와 네이버스토어의 경우 개발자를 위한 Open API에 상품 검색 기능이 존재했다. API를 이용하여 상품 검색 기능을 구현할 경우 장점은 다음과 같다.
- 크롤링보다 빠르다.
- 공식이다. (즉 쇼핑몰 운영 정책에 위반될 여지가 없다.)
- 크롤링보다 편리하다.

다만 API는 쇼핑몰마다 그 방식과 사용법이 다르기에, 각 쇼핑몰에 맞추어 일일이 구현해야 한다는 단점이 있다. 스크래핑을 이용할 경우 쇼핑몰별 상품 카드의 셀렉터만 수정해주면 된다는 점과 대조적이다.

### 2. 다나와 결과 활용
다나와는 기본적으로 이 프로젝트와 굉장히 비슷한 목적을 가지고 있다. 특정 상품에 대해 쇼핑몰별 최저가를 찾아준다. 그 데이터를 Findit이 활용할 수도 있을 것이다. 다나와는 위의 두 쇼핑몰과는 다르게 느슨한 스크래핑 정책을 가지고 있는 것으로 알고 있다. 쿠팡과 지마켓처럼 스크래핑이 불가능한 쇼핑몰에 대해서는 불가피하게 다나와의 상품가격 데이터를 가져오는 방식으로 구현하는 것이 차선책일 것 같다.

물론 '그럴거면 굳이 Findit을 쓸 필요가 있나? 다나와를 쓰지'라는 근본적인 의문점이 생긴다. 다나와 서비스와 이 프로젝트의 차별점으로써 내세울 만한 것을 생각해 볼 필요가 있을 것 같다. 

다음엔 11번가와 네이버스토어의 API로 상품 검색 기능을 구현하는 과정에 대해 포스팅하게 될 것 같다. 